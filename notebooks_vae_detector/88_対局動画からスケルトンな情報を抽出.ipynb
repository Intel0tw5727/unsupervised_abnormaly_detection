{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "- Poseを使った異常検知をMNISTでテストしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Model, model_from_json\n",
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend\n",
    "\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "session = tf.Session(config=config)\n",
    "tensorflow_backend.set_session(session)\n",
    "\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import socket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 棋譜の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isFujii = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/kif/AbemaTV_Tournament_Final1_analized.kif\", encoding=\"utf-8\") as f:\n",
    "    kif = f.readlines()\n",
    "shogi_df = pd.DataFrame([int(k.split(\" \")[1]) for k in kif if \"*##\" in k], columns=[\"score\"])\n",
    "\n",
    "if not isFujii:# 後手番なら評価値を逆にする\n",
    "    shogi_df[\"score\"] = shogi_df[\"score\"] * -1\n",
    "shogi_df['score_label'] = shogi_df[\"score\"].map(lambda x: x//300)\n",
    "shogi_df[\"hand_number\"] = shogi_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数や変数を読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openposeのやつ\n",
    "from DeNAPose.pose_detector import PoseDetector, draw_person_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skelton(img, isFujii, pose_detector, pose_thresh):\n",
    "    pose_rect = cv2.flip(img[678:968, 45:565].copy(), 1) if isFujii else img[220:510, 1340:1860].copy()\n",
    "\n",
    "    pose_keypoints, scores = pose_detector(pose_rect)\n",
    "    if len(np.where(scores > pose_thresh)[0]) == 0:\n",
    "        # 真っ白な画像を作成\n",
    "        img_skel = (np.ones_like(pose_rect) / 255).astype(np.float32)\n",
    "        img_player = (np.ones_like(pose_rect) / 255).astype(np.float32)\n",
    "    else:    \n",
    "        img_skel = draw_person_pose(np.full_like(pose_rect, 255), pose_keypoints[np.where(scores > pose_thresh)])\n",
    "        \n",
    "        img_player = draw_person_pose(pose_rect, pose_keypoints[np.where(scores > pose_thresh)])\n",
    "#         if isFujii:\n",
    "#             pose_rect = cv2.filp(img_player, 1)\n",
    "#         else:\n",
    "#             pose_rect = img_player\n",
    "\n",
    "    return img_player, img_skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    size = 32\n",
    "  \n",
    "    # openposeのミスをできるだけ前処理で落とす\n",
    "    try:\n",
    "        x, w, y, h = trimming(img)\n",
    "        margin = 16\n",
    "        img_trim = img[y-margin:h+margin, x-margin:w+margin]\n",
    "        height, width = img_trim.shape\n",
    "        \n",
    "        if (height < 100) or (width < 100):\n",
    "            return np.zeros((size, size))\n",
    "        \n",
    "        # 膨張処理\n",
    "        kernel = np.ones((6,6),np.uint8)\n",
    "        img_trim = cv2.bitwise_not(img_trim) # 白(255)を膨張させるため反転\n",
    "        img_dil = cv2.dilate(img_trim,kernel,iterations = 1)\n",
    "        \n",
    "        img_dil = cv2.resize(img_dil, (size, size))\n",
    "        \n",
    "        # 2値化\n",
    "        _,img_bin = cv2.threshold(img_dil,0,1,cv2.THRESH_BINARY)\n",
    "        \n",
    "        return img_bin * 255.0 \n",
    "    except:\n",
    "        return np.zeros((size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimming(img):\n",
    "    mask = img < 255\n",
    "    x = np.where(np.sum(mask, axis=0) > 1)[0]\n",
    "    y = np.where(np.sum(mask, axis=1) > 1)[0]\n",
    "    \n",
    "    x_min, x_max = x[0], x[-1]\n",
    "    y_min, y_max = y[0], y[-1]\n",
    "    return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_bytes_decode_img():\n",
    "    with BytesIO() as output:\n",
    "        func()\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_graph = cv2.imdecode(img_array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_player(img, mappable, img_diff):\n",
    "    height, width = img.shape[:2]\n",
    "    jet_color = np.array(cm.jet(img_diff/250)[:3])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    with BytesIO() as output:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"shogi_player\")\n",
    "        plt.imshow(cv2.rectangle(img, (0, 0), (width-1, height-1), jet_color * 255, int(min(height, width)/10)))\n",
    "        plt.colorbar(mappable, fraction=0.026, pad=0.04)\n",
    "        plt.savefig(output, format=\"PNG\")\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_player = cv2.imdecode(img_array, 3)\n",
    "    \n",
    "    return img_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_img(df, hands):\n",
    "    with BytesIO() as output:\n",
    "        df.score.plot()\n",
    "        plt.plot(df.index[hands], df.score[hands], \"ro\", markersize=4)\n",
    "        plt.title(\"sup_or_inf\")\n",
    "        plt.savefig(output, format=\"PNG\")\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_graph = cv2.imdecode(img_array, 3)\n",
    "    \n",
    "    return img_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compare_pose(img_orig, img_pred):\n",
    "    with BytesIO() as output:\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(5, 3))\n",
    "        for i, (img, title) in enumerate(zip([img_orig, img_pred], [\"original\", \"prediction\"])):  \n",
    "            ax[i].imshow(img, cmap=\"gray\")\n",
    "            ax[i].set_title(title)\n",
    "        plt.savefig(output, format=\"PNG\")\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_pose = cv2.imdecode(img_array, 3)\n",
    "    \n",
    "    return img_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画に対してリアルタイムに評価する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract(cap):\n",
    "    pose_thresh = 4 # openposeによるスケルトン抽出時の信頼度スコアのしきい値\n",
    "    sampling_frame_rate = 6\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))//sampling_frame_rate\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\")\n",
    "    video_orig = cv2.VideoWriter(\"../pose_detection_result_orig.mp4\", fourcc, 1.0, (520, 290))\n",
    "    video_skel = cv2.VideoWriter(\"../pose_detection_result_skel.mp4\", fourcc, 1.0, (32, 32))\n",
    "    \n",
    "    # openposeモデルの読み込み\n",
    "    pose_detector = PoseDetector(\"posenet\", \"../utils/DeNAPose/models/coco_posenet.npz\", device=0)\n",
    "    \n",
    "    for i in tqdm(range(total)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, sampling_frame_rate*i)\n",
    "        frame = cap.read()[1]\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img_orig, img_skelton = extract_skelton(frame, False, pose_detector, pose_thresh)\n",
    "        img_pose = preprocess(cv2.cvtColor(img_skelton, cv2.COLOR_RGB2GRAY)) / 255\n",
    "         \n",
    "        img_orig = cv2.cvtColor(img_orig.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "        img_pose = cv2.cvtColor((img_pose * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        video_orig.write(img_orig)\n",
    "        video_skel.write(img_pose)\n",
    "    video_orig.release()\n",
    "    video_skel.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def make_abnormal_detect(cap_orig, cap_skel):\n",
    "    total = int(cap_orig.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    norm = Normalize(vmin=0, vmax=250)\n",
    "    cmap = cm.get_cmap(\"jet\")\n",
    "    mappable = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    mappable._A = []\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\")\n",
    "    video = cv2.VideoWriter(\"../abnormal_pose_detection_result.mp4\", fourcc, 20.0, (720, 480))\n",
    "    \n",
    "    # アーキテクチャの読み込み\n",
    "    with open(\"../output/conv_vae_model_20190107_074233.json\") as f:\n",
    "        json_string = f.readline()\n",
    "\n",
    "    vae = model_from_json(json_string)\n",
    "    vae.load_weights(\"../output/conv_vae_best_param_20190107_074233.hdf5\")\n",
    "    print(\"Loaded Models and Params\")\n",
    "    \n",
    "    imgs_orig, imgs_pose = [], []\n",
    "    for i in tqdm(range(total)):\n",
    "        img_orig = cap_orig.read()[1]\n",
    "        img_pose = cv2.cvtColor(cap_skel.read()[1], cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        imgs_orig.append(img_orig)\n",
    "        imgs_pose.append(img_pose)\n",
    "        \n",
    "    imgs_orig, imgs_pose = np.array(imgs_orig), np.array(imgs_pose)\n",
    "    h, w = img_pose.shape[:2]\n",
    "    imgs_pose = imgs_pose.reshape(total, h, w, 1)\n",
    "    imgs_pred = vae.predict(imgs_pose)\n",
    "    \n",
    "    for img_orig, img_pose, img_pred in tqdm(zip(imgs_orig, imgs_pose, imgs_pred)):\n",
    "        canvas = np.ones((480, 720, 3), np.uint8) * 255\n",
    "        h_canvas, w_canvas = canvas.shape[:2]\n",
    "        \n",
    "        h, w = img_pose.shape[:2]\n",
    "        img_pred = img_pred.reshape(32,32)\n",
    "        img_pose = img_pose.reshape(32,32)\n",
    "        \n",
    "        img_diff = np.sum(np.abs(img_pose - img_pred))\n",
    "        \n",
    "        # openpose結果の出力\n",
    "        #img_heat = cv2.cvtColor(make_player(img_orig, mappable, img_diff), cv2.COLOR_BGR2RGB)\n",
    "        img_heat = make_player(img_orig, mappable, img_diff)\n",
    "        del img_orig\n",
    "        x, w, y, h = trimming(img_heat)\n",
    "        img_heat = img_heat[y-16:h+16, x-16:w+16]\n",
    "        h_orig, w_orig = img_heat.shape[:2]\n",
    "\n",
    "        canvas[0:h_orig, 0:w_orig] = img_heat\n",
    "        \n",
    "        img_compare = make_compare_pose(img_pose, img_pred)\n",
    "        del img_pose, img_pred\n",
    "\n",
    "        x, w, y, h = trimming(cv2.cvtColor(img_compare, cv2.COLOR_BGR2GRAY))\n",
    "        img_compare = img_compare[y-16:h+16, x-16:w+16]\n",
    "        h_comp, w_comp = img_compare.shape[:2]\n",
    "\n",
    "        #canvas[h_canvas - h_graph - 3:h_canvas, w_graph:w_graph + img_compare.shape[1]] = img_compare\n",
    "        canvas[h_canvas - h_comp:h_canvas, 0:w_comp] = img_compare\n",
    "        \n",
    "        video.write(canvas)\n",
    "        # サーバーへ転送\n",
    "        #canvas_string = canvas.tostring()\n",
    "        #soc_cli.send(canvas_string)\n",
    "        del canvas\n",
    "        #del canvas_string\n",
    "        del img_heat\n",
    "        del img_compare\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../pose_detection_result_orig.mp4': No such file or directory\r\n",
      "rm: cannot remove '../pose_detection_result_skel.mp4': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm ../pose_detection_result_orig.mp4 ../pose_detection_result_skel.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture(\"../data/final1_labeling.mp4\")\n",
    "extract(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cap_orig = cv2.VideoCapture(\"../pose_detection_result_orig.mp4\")\n",
    "cap_skel = cv2.VideoCapture(\"../pose_detection_result_skel.mp4\")\n",
    "make_abnormal_detect(cap_orig, cap_skel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cap):\n",
    "    pose_thresh = 5 # openposeによるスケルトン抽出時の信頼度スコアのしきい値\n",
    "    sampling_frame_rate = 6\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))//sampling_frame_rate\n",
    "    img_diff\n",
    "\n",
    "    # ヒートマップ用初期パラメータ\n",
    "    norm = Normalize(vmin=0, vmax=250)\n",
    "    cmap = cm.get_cmap(\"jet\")\n",
    "    mappable = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    mappable._A = []\n",
    "    \n",
    "    # 転送用設定\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((\"10.0.2.159\", 9898))\n",
    "    print(\"Waiting...\")\n",
    "    s.listen(1)\n",
    "    soc_cli, addr = s.accept()\n",
    "    print(\"Connected {}\".format(addr))\n",
    "    \n",
    "    soc_sev = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    soc_sev.connect((\"10.10.12.214\", 8989))\n",
    "    print(\"Connected\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\")\n",
    "    video = cv2.VideoWriter(\"../abnormal_pose_detection_result.mp4\", fourcc, 6.0, (720, 480))\n",
    "    \n",
    "    # openposeモデルの読み込み\n",
    "    pose_detector = PoseDetector(\"posenet\", \"../utils/DeNAPose/models/coco_posenet.npz\", device=0)\n",
    "    \n",
    "    # アーキテクチャの読み込み\n",
    "    with open(\"../output/conv_vae_model_20190107_074233.json\") as f:\n",
    "        json_string = f.readline()\n",
    "\n",
    "    vae = model_from_json(json_string)\n",
    "    vae.load_weights(\"../output/conv_vae_best_param_20190107_074233.hdf5\")\n",
    "    \n",
    "    #while cap.isOpened():\n",
    "    for i in tqdm(range(total)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, sampling_frame_rate*i)\n",
    "        # 動画用に書き出すデータを1枚のcanvasにまとめる\n",
    "        canvas = np.ones((480, 720, 3), np.uint8) * 255\n",
    "        h_canvas, w_canvas = canvas.shape[:2]\n",
    "        \n",
    "        frame = cap.read()[1]\n",
    "        try:\n",
    "            img_orig, img_skelton = extract_skelton(frame, False, pose_detector, pose_thresh)\n",
    "        except:\n",
    "            continue\n",
    "        del frame\n",
    "        img_pose = preprocess(cv2.cvtColor(img_skelton, cv2.COLOR_RGB2GRAY)) / 255\n",
    "        del img_skelton\n",
    "        \n",
    "        h, w = img_pose.shape\n",
    "        img_pose = img_pose.reshape(1, h, w, 1)\n",
    "        img_pred = vae.predict(img_pose)[0].reshape(32,32)\n",
    "        img_pose = img_pose[0].reshape(32,32)\n",
    "        \n",
    "        img_diff = np.sum(np.abs(img_pose - img_pred))\n",
    "        \n",
    "        # openpose結果の出力\n",
    "        #img_heat = cv2.cvtColor(make_player(img_orig, mappable, img_diff), cv2.COLOR_BGR2RGB)\n",
    "        img_heat = make_player(img_orig, mappable, img_diff)\n",
    "        del img_orig\n",
    "        x, w, y, h = trimming(img_heat)\n",
    "        img_heat = img_heat[y-16:h+16, x-16:w+16]\n",
    "        h_orig, w_orig = img_heat.shape[:2]\n",
    "\n",
    "        canvas[0:h_orig, 0:w_orig] = img_heat\n",
    "        \n",
    "        # 対局中の局面評価を出力\n",
    "#         img_graph = make_graph_img(shogi_df[np.abs(shogi_df.score) < 9999], 0)\n",
    "\n",
    "#         h_result = h_canvas - img_heat.shape[0]\n",
    "#         h_graph, w_graph = img_graph.shape[:2]\n",
    "#         rate = round(h_result / h_graph, 2)\n",
    "#         img_graph = cv2.resize(img_graph.copy(), (int(w_graph*rate), h_result))\n",
    "#         h_graph, w_graph = img_graph.shape[:2]\n",
    "\n",
    "#         canvas[h_canvas - h_graph:h_canvas, 0:w_graph] = img_graph\n",
    "#         del img_graph\n",
    "        \n",
    "        # 入力データと予測データの比較画像を出力\n",
    "        img_compare = make_compare_pose(img_pose, img_pred)\n",
    "        del img_pose, img_pred\n",
    "\n",
    "        x, w, y, h = trimming(cv2.cvtColor(img_compare, cv2.COLOR_BGR2GRAY))\n",
    "        img_compare = img_compare[y-16:h+16, x-16:w+16]\n",
    "        h_comp, w_comp = img_compare.shape[:2]\n",
    "\n",
    "        #canvas[h_canvas - h_graph - 3:h_canvas, w_graph:w_graph + img_compare.shape[1]] = img_compare\n",
    "        canvas[h_canvas - h_comp:h_canvas, 0:w_comp] = img_compare\n",
    "        \n",
    "        #video.write(canvas)\n",
    "        # サーバーへ転送\n",
    "        canvas_string = canvas.tostring()\n",
    "        soc_cli.send(canvas_string)\n",
    "        del canvas\n",
    "        del canvas_string\n",
    "        del img_heat\n",
    "        del img_compare\n",
    "        \n",
    "        soc_sev.recv(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting...\n",
      "Connected ('10.10.12.214', 51113)\n",
      "Connected\n",
      "Loading the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a4324df59f40468da6e999ddd7b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=542), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/Desktop/Research/utils/DeNAPose/pose_detector.py:147: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  paf_in_edge = np.hstack([paf[0][np.hsplit(integ_points, 2)], paf[1][np.hsplit(integ_points, 2)]])\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "/home/nvidia/.pyenv/versions/3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n",
      "/home/nvidia/.pyenv/versions/3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py:10: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"../data/final1_labeling.mp4\")\n",
    "main(cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
