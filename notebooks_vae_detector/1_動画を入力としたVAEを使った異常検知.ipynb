{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "- Poseを使った異常検知をMNISTでテストしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Layer, BatchNormalization, Dropout, RepeatVector\n",
    "from keras.layers import Lambda, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.layers import Reshape, Conv2DTranspose, UpSampling2D, TimeDistributed\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import time\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbemaTV_Tournament_Final1\t fujii_vs_kondo_Ablock_2_2_1080p\r\n",
      "AbemaTV_Tournament_Final2\t fujii_vs_takami_semi_1_1080p\r\n",
      "AbemaTV_Tournament_Final3\t fujii_vs_takami_semi_2_1080p\r\n",
      "fujii_vs_kondo_Ablock_1_1080p\t fujii_vs_takami_semi_3_1080p\r\n",
      "fujii_vs_kondo_Ablock_2_1_1080p\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/shogi_pose_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirs_path = sorted(glob(\"../data/shogi_pose_dataset/AbemaTV_Tournament_Final*\"))\n",
    "data_path = sorted(glob(\"../data/shogi_pose_dataset/AbemaTV_Tournament_Final1/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag = True\n",
    "# for dir_path in dirs_path:\n",
    "#     if flag:\n",
    "#         data_path = sorted(glob(os.path.join(dir_path, \"*\")))\n",
    "#         flag = False\n",
    "#     else:\n",
    "#         data_path += sorted(glob(os.path.join(dir_path, \"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    cv2.setNumThreads(0)\n",
    "    img = cv2.imread(path, 0)\n",
    "    size = 32\n",
    "    \n",
    "    \n",
    "    # openposeのミスをできるだけ前処理で落とす\n",
    "    try:\n",
    "        x, w, y, h = trimming(img)\n",
    "        margin = 16\n",
    "        img_trim = img[y-margin:h+margin, x-margin:w+margin]\n",
    "        height, width = img_trim.shape\n",
    "        \n",
    "        if (height < 150) or (width < 150):\n",
    "            return np.zeros((size, size))\n",
    "        \n",
    "        # 膨張処理\n",
    "        kernel = np.ones((6,6),np.uint8)\n",
    "        img_trim = cv2.bitwise_not(img_trim) # 白(255)を膨張させるため反転\n",
    "        img_dil = cv2.dilate(img_trim,kernel,iterations = 1)\n",
    "        \n",
    "        img_dil = cv2.resize(img_dil, (size, size))\n",
    "        \n",
    "        # 2値化\n",
    "        _,img_bin = cv2.threshold(img_dil,0,1,cv2.THRESH_BINARY)\n",
    "        \n",
    "        return img_bin * 255.0\n",
    "    except:\n",
    "        return np.zeros((size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimming(img):\n",
    "    mask = img < 255\n",
    "    x = np.where(np.sum(mask, axis=0) > 1)[0]\n",
    "    y = np.where(np.sum(mask, axis=1) > 1)[0]\n",
    "    \n",
    "    x_min, x_max = x[0], x[-1]\n",
    "    y_min, y_max = y[0], y[-1]\n",
    "    return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f07a0a1c57437bb644ba2a5187e46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10322), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee18155fb9dc4b5a93a00462e448eacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "with tqdm(total=len(data_path)) as pbar:\n",
    "    with Pool(mp.cpu_count()) as p:\n",
    "        for img in tqdm(p.imap_unordered(read_img, data_path)):\n",
    "            imgs.append(img)\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を4フレームごとに分割\n",
    "- 4フレーム(1秒)のデータを5フレームごとにスライドしてデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a800b054b343779ce6d84d0d0548c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2063), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stride = 5\n",
    "time = 4\n",
    "\n",
    "vids = []\n",
    "for idx in tqdm(range((len(imgs) // 5) - 1)):\n",
    "    vids.append(np.array(imgs[idx * stride: time + idx * stride]))\n",
    "    \n",
    "del imgs\n",
    "del data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vids[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2063, 10, 32, 32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(np.array(vids), np.array([0 for _ in range(len(vids))]), test_size=0.3, random_state=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[np.where(y_train == 0)[0]]\n",
    "# y_train = [0 for _ in range(len(np.where(y_train == 0)[0]))]\n",
    "# X_test = X_test[np.where(y_test == 0)[0]]\n",
    "# y_test = [0 for _ in range(len(np.where(y_test == 0)[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1444, 10, 32, 32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619, 10, 32, 32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1444, 10, 32, 32, 1) (616, 10, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# 前処理\n",
    "X_train = X_train[:X_train.shape[0] // batch_size * batch_size].astype(\"float32\") / 255\n",
    "X_test = X_test[:X_test.shape[0] // batch_size * batch_size].astype(\"float32\") / 255\n",
    "\n",
    "train_num, time, height, width = X_train.shape\n",
    "test_num = X_test.shape[0]\n",
    "\n",
    "X_train = X_train.reshape(train_num, time, height, width, 1)\n",
    "X_test = X_test.reshape(test_num, time, height, width, 1)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "filters = 32\n",
    "add_filter_size = 32\n",
    "n_hidden = 512\n",
    "z_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    eps = K.random_normal(shape=(K.shape(z_mean)[0],\n",
    "                                K.int_shape(z_mean)[1]), \n",
    "                          mean=0., \n",
    "                          stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 32, 32, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)     (None, 10, 16, 16, 3 38144       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 16, 16, 3 128         conv_lst_m2d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)     (None, 10, 8, 8, 64) 221440      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 8, 8, 64) 256         conv_lst_m2d_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)     (None, 10, 4, 4, 96) 553344      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10, 4, 4, 96) 384         conv_lst_m2d_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 15360)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          7864832     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,682,628\n",
      "Trainable params: 8,681,220\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# エンコーダ\n",
    "inputs = Input(shape=(X_train.shape[1:]))\n",
    "x = inputs\n",
    "\n",
    "for _ in range(3):\n",
    "    x = ConvLSTM2D(filters=filters,\n",
    "               kernel_size=(kernel_size, kernel_size),\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               return_sequences=True)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    filters += add_filter_size\n",
    "\n",
    "shape_before_flat = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "\n",
    "x = Dense(n_hidden, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "z_mean = Dense(z_dim, name=\"z_mean\")(x)\n",
    "z_log_var = Dense(z_dim, name=\"z_log_var\")(x)\n",
    "z = Lambda(sampling, output_shape=(z_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15360)             7879680   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 10, 4, 4, 96)      0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, 10, 4, 4, 96)      663936    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 8, 8, 96)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 10, 8, 8, 96)      384       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  (None, 10, 8, 8, 64)      368896    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 16, 16, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 10, 16, 16, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, 10, 16, 16, 32)    110720    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 10, 32, 32, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 32, 32, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  (None, 10, 32, 32, 1)     1192      \n",
      "=================================================================\n",
      "Total params: 9,028,776\n",
      "Trainable params: 9,027,368\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# デコーダ\n",
    "latent_inputs = Input(shape=(z_dim,), name=\"z_sampling\")\n",
    "\n",
    "z_decoded = Dense(n_hidden, activation=\"relu\")(latent_inputs)\n",
    "z_decoded = BatchNormalization()(z_decoded)\n",
    "\n",
    "z_decoded = Dense(np.prod(shape_before_flat[1:]), activation=\"relu\")(z_decoded)\n",
    "z_decoded = Reshape(shape_before_flat[1:])(z_decoded)\n",
    "\n",
    "# z_decoded = RepeatVector(time)(z_decoded)\n",
    "\n",
    "# # z_decode = Conv3D(filters=1, kernel_size=(kernel_size,kernel_size,kernel_size),\n",
    "# #           activation=\"relu\", padding=\"same\", data_format=\"channels_last\")(z_decoded)\n",
    "\n",
    "for _ in range(3):\n",
    "    filters -= add_filter_size\n",
    "    z_decoded = ConvLSTM2D(filters=filters,\n",
    "               kernel_size=(kernel_size, kernel_size),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               return_sequences=True)(z_decoded)\n",
    "    z_decoded = TimeDistributed(UpSampling2D(size=(2,2)))(z_decoded)\n",
    "    z_decoded = BatchNormalization()(z_decoded)\n",
    "\n",
    "outputs = ConvLSTM2D(filters=1,\n",
    "               kernel_size=(kernel_size, kernel_size),\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               return_sequences=True)(z_decoded)\n",
    "    \n",
    "#decoder = Model(latent_inputs, [outputs1, outputs2], name=\"decoder\")\n",
    "decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "# outputs_mu, outputs_sigma_2 = decoder(encoder(inputs)[2])\n",
    "# vae = Model(inputs, [outputs_mu, outputs_sigma_2], name='vae_mlp')\n",
    "outputs_decoder = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs_decoder, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kl_loss\n",
    "    - カルバック・ライブラー情報量\n",
    "    - 確率論と情報理論を組み合わせた２つの確率分布がどの程度似ているかを表す尺度\n",
    "\n",
    "$$ loss = \\frac{1}{2} \\sum^{N_z}_{j=1} (\\mu^{2}_{z_j} + \\sigma^{2}_{z_j} - \\log{\\sigma^2_{z_j} - 1}) \\\\ = \\frac{1}{2} \\cdot \\sum^{N_z}_{j=1} (\\mu^{2}_{z_j} + \\exp(\\log{\\sigma}) - \\log{\\sigma} - 1)?$$\n",
    "\n",
    "> https://qiita.com/shinmura0/items/811d01384e20bfd1e035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss\n",
    "# m_vae_loss = (K.flatten(inputs) - K.flatten(outputs_mu))**2 / K.flatten(outputs_sigma_2)\n",
    "# m_vae_loss = 0.5 * K.sum(m_vae_loss)\n",
    "\n",
    "# a_vae_loss = K.log(2 * np.pi * K.flatten(outputs_sigma_2))\n",
    "# a_vae_loss = 0.5 * K.sum(a_vae_loss)\n",
    "\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.mean(K.sum(kl_loss, axis=-1))\n",
    "kl_loss *= -0.5\n",
    "\n",
    "reconstruction_loss = K.sum(binary_crossentropy(K.flatten(inputs), K.flatten(outputs_decoder))) * width * height\n",
    "\n",
    "vae_loss = reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 8682628   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 10, 32, 32, 1)     9028776   \n",
      "=================================================================\n",
      "Total params: 17,711,404\n",
      "Trainable params: 17,708,588\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer=\"rmsprop\")\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboardによる監視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoardImage(Callback):\n",
    "    def __init__(self, model, tag, now):\n",
    "        super().__init__() \n",
    "        self._model = model\n",
    "        self._tag = tag\n",
    "        self._now = now\n",
    "        \n",
    "    def tf_summary_image(self, img):\n",
    "        import io\n",
    "        from PIL import Image\n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "        height, width, channel = img.shape if len(img.shape) == 3 else (img.shape[0], img.shape[1], 1)\n",
    "        \n",
    "        image = Image.fromarray(img)\n",
    "        with io.BytesIO() as output:\n",
    "            \n",
    "            image.save(output, format=\"PNG\")\n",
    "            image_string = output.getvalue()\n",
    "        return tf.Summary.Image(height=height,\n",
    "                               width=width,\n",
    "                               colorspace=channel,\n",
    "                               encoded_image_string=image_string)\n",
    "\n",
    "    def decode_image(self):\n",
    "        n = 15 # figure with 15x15 digits\n",
    "        digit_size = 48\n",
    "        figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "        grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) \n",
    "        grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "        for i, yi in enumerate(grid_x):\n",
    "            for j, xi in enumerate(grid_y):\n",
    "                z_sample = np.array([[xi, yi]])\n",
    "                x_decoded = self._model.predict(z_sample)\n",
    "                digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "                figure[i * digit_size: (i + 1) * digit_size,\n",
    "                       j * digit_size: (j + 1) * digit_size] = digit\n",
    "        \n",
    "        figure *= 255\n",
    "        return figure\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Do something to the image\n",
    "        pose_dist = self.decode_image()\n",
    "        image = self.tf_summary_image(pose_dist)\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=self._tag, image=image)])\n",
    "        with tf.summary.FileWriter('/home/nvidia/Desktop/Research/tmp/vaeimg_{}'.format(str_now)) as writer:\n",
    "            writer.add_summary(summary, epoch)\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190106_122351\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "str_now = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "es_cb = EarlyStopping(monitor='loss', patience=20, mode=\"auto\")\n",
    "tb_cb = TensorBoard(log_dir='/home/nvidia/Desktop/Research/tmp/vaelog_{}'.format(str_now),\n",
    "                    write_images=True, \n",
    "                    write_grads=True, \n",
    "                    write_graph=True)\n",
    "mc_cb = ModelCheckpoint(\"/home/nvidia/Desktop/Research/output/vae_best_param.hdf5\",\n",
    "                       monitor=\"val_loss\",\n",
    "                       verbose=1,\n",
    "                       save_best_only=True,\n",
    "                       mode=\"auto\")\n",
    "tbi_cb = TensorBoardImage(decoder, \"pose distribution\", str_now)\n",
    "print(str_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbks = [es_cb, tb_cb, mc_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n",
      "|       BatchNormalization|      2456|\n",
      "|                 Callback|      1016|\n",
      "|                   Conv2D|      1016|\n",
      "|          Conv2DTranspose|      1016|\n",
      "|                   Conv3D|      1016|\n",
      "|               ConvLSTM2D|      2456|\n",
      "|                    Dense|      2456|\n",
      "|                  Dropout|      1016|\n",
      "|            EarlyStopping|      1688|\n",
      "|                  Flatten|      1688|\n",
      "|                       In|       344|\n",
      "|                    Input|       136|\n",
      "|                        K|        80|\n",
      "|                   Lambda|      1688|\n",
      "|                    Layer|      1016|\n",
      "|             MaxPooling2D|      1016|\n",
      "|                    Model|       888|\n",
      "|          ModelCheckpoint|      1304|\n",
      "|                      Out|       288|\n",
      "|                     Pool|        64|\n",
      "|             RepeatVector|      1016|\n",
      "|                  Reshape|      1688|\n",
      "|              TensorBoard|      1688|\n",
      "|         TensorBoardImage|      1304|\n",
      "|          TimeDistributed|      1688|\n",
      "|             UpSampling2D|      1688|\n",
      "|                   X_test|       160|\n",
      "|                  X_train|       160|\n",
      "|          add_filter_size|        28|\n",
      "|               batch_size|        28|\n",
      "|      binary_crossentropy|       136|\n",
      "|                     cbks|        88|\n",
      "|                      cv2|        80|\n",
      "|                 datetime|        80|\n",
      "|                  decoder|        56|\n",
      "|                  encoder|        56|\n",
      "|                    es_cb|        56|\n",
      "|                     exit|        56|\n",
      "|                  filters|        28|\n",
      "|              get_ipython|        64|\n",
      "|                     glob|       136|\n",
      "|                   height|        28|\n",
      "|                      idx|        28|\n",
      "|                      img|       112|\n",
      "|                   inputs|        56|\n",
      "|              kernel_size|        28|\n",
      "|                  kl_loss|        56|\n",
      "|            latent_inputs|        56|\n",
      "|                    mc_cb|        56|\n",
      "|                    mnist|        80|\n",
      "|                       mp|        80|\n",
      "|                 n_hidden|        28|\n",
      "|                     norm|        56|\n",
      "|                      now|        48|\n",
      "|                       np|        80|\n",
      "|                       os|        80|\n",
      "|                  outputs|        56|\n",
      "|          outputs_decoder|        56|\n",
      "|                        p|        56|\n",
      "|                     pbar|        56|\n",
      "|                      plt|        80|\n",
      "|                     quit|        56|\n",
      "|                 read_img|       136|\n",
      "|      reconstruction_loss|        56|\n",
      "|                 sampling|       136|\n",
      "|        shape_before_flat|        88|\n",
      "|                  str_now|        64|\n",
      "|                   stride|        28|\n",
      "|                      sys|        80|\n",
      "|                    tb_cb|        56|\n",
      "|                   tbi_cb|        56|\n",
      "|                 test_num|        28|\n",
      "|                       tf|        80|\n",
      "|                     time|        28|\n",
      "|                     tqdm|       136|\n",
      "|                train_num|        28|\n",
      "|         train_test_split|       136|\n",
      "|                 trimming|       136|\n",
      "|                      vae|        56|\n",
      "|                 vae_loss|        56|\n",
      "|                     vids|     18672|\n",
      "|                    width|        28|\n",
      "|                        x|        56|\n",
      "|                   y_test|      5048|\n",
      "|                  y_train|     11648|\n",
      "|                        z|        56|\n",
      "|                z_decoded|        56|\n",
      "|                    z_dim|        28|\n",
      "|                z_log_var|        56|\n",
      "|                   z_mean|        56|\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\"):\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights(\"../output/vidvae_init_param_{}.hdf5\".format(str_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.load_weights(\"../output/vae_best_param.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1444 samples, validate on 616 samples\n",
      "Epoch 1/100\n",
      "1444/1444 [==============================] - 506s 351ms/step - loss: 306.8948 - val_loss: 292.8859\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 292.88591, saving model to /home/nvidia/Desktop/Research/output/vae_best_param.hdf5\n",
      "Epoch 2/100\n",
      "1076/1444 [=====================>........] - ETA: 1:47 - loss: 274.0076"
     ]
    }
   ],
   "source": [
    "history = vae.fit(X_train,\n",
    "        shuffle=True,\n",
    "        epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, None),\n",
    "        verbose=1,\n",
    "        callbacks=cbks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = vae.to_json()\n",
    "with open(\"../output/vidvae_model_{}.json\".format(str_now), \"w\") as json_model:\n",
    "    json_model.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights(\"../output/vidvae_best_param_{}.hdf5\".format(str_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder.predict(np.array([[0.5, 0.5]]))[0].reshape(10,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=10, figsize=(18,10))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(pred[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "# 0 に対して分布を見てみる\n",
    "n = 15 # figure with 15x15 digits\n",
    "digit_size = 48\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) \n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "VAEによる1種類の生成モデルを学習させることで1枚の画像の分布を得ることができるのでこれを用いて画像比較ができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
