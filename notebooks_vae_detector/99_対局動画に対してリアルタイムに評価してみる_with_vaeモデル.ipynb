{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "- Poseを使った異常検知をMNISTでテストしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Model, model_from_json\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize(vmin=0, vmax=250)\n",
    "cmap = cm.get_cmap(\"jet\")\n",
    "mappable = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "mappable._A = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_m = np.array([np.ones((28,280,3)) * cm.jet((100-i)/100)[:3] for i in range(100)]).reshape(28 * 100,280, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.22156863, 1.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_color = np.array(cm.jet(np.random.rand(1)[0])[:3])# * 255\n",
    "jet_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f1c458860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "height, width = color_m.shape[:2]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(color_m)\n",
    "plt.title(\"shogi_player\")\n",
    "plt.imshow(cv2.rectangle(color_m, (0, 0), (width-1, height-1), jet_color, int(min(height, width)/10)))\n",
    "plt.colorbar(mappable, fraction=0.026, pad=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 棋譜の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isFujii = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/kif/AbemaTV_Tournament_Final1_analized.kif\", encoding=\"utf-8\") as f:\n",
    "    kif = f.readlines()\n",
    "shogi_df = pd.DataFrame([int(k.split(\" \")[1]) for k in kif if \"*##\" in k], columns=[\"score\"])\n",
    "\n",
    "if not isFujii:# 後手番なら評価値を逆にする\n",
    "    shogi_df[\"score\"] = shogi_df[\"score\"] * -1\n",
    "shogi_df['score_label'] = shogi_df[\"score\"].map(lambda x: x//300)\n",
    "shogi_df[\"hand_number\"] = shogi_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数や変数を読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openposeのやつ\n",
    "from DeNAPose.pose_detector import PoseDetector, draw_person_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skelton(img, isFujii, pose_detector, pose_thresh):\n",
    "    pose_rect = cv2.flip(img[678:968, 45:565].copy(), 1) if isFujii else img[220:510, 1340:1860].copy()\n",
    "\n",
    "    pose_keypoints, scores = pose_detector(pose_rect)\n",
    "    if len(np.where(scores > pose_thresh)[0]) == 0:\n",
    "        # 真っ白な画像を作成\n",
    "        img_skel = np.zeros_like(pose_rect)\n",
    "    else:    \n",
    "        img_skel = draw_person_pose(np.full_like(pose_rect, 255), pose_keypoints[np.where(scores > pose_thresh)])\n",
    "        \n",
    "        img_player = draw_person_pose(pose_rect, pose_keypoints[np.where(scores > pose_thresh)])\n",
    "#         if isFujii:\n",
    "#             pose_rect = cv2.filp(img_player, 1)\n",
    "#         else:\n",
    "#             pose_rect = img_player\n",
    "\n",
    "    return img_player, img_skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    size = 32\n",
    "  \n",
    "    # openposeのミスをできるだけ前処理で落とす\n",
    "    try:\n",
    "        x, w, y, h = trimming(img)\n",
    "        margin = 16\n",
    "        img_trim = img[y-margin:h+margin, x-margin:w+margin]\n",
    "        height, width = img_trim.shape\n",
    "        \n",
    "        if (height < 100) or (width < 100):\n",
    "            return np.zeros((size, size))\n",
    "        \n",
    "        # 膨張処理\n",
    "        kernel = np.ones((6,6),np.uint8)\n",
    "        img_trim = cv2.bitwise_not(img_trim) # 白(255)を膨張させるため反転\n",
    "        img_dil = cv2.dilate(img_trim,kernel,iterations = 1)\n",
    "        \n",
    "        img_dil = cv2.resize(img_dil, (size, size))\n",
    "        \n",
    "        # 2値化\n",
    "        _,img_bin = cv2.threshold(img_dil,0,1,cv2.THRESH_BINARY)\n",
    "        \n",
    "        return img_bin * 255.0 \n",
    "    except:\n",
    "        return np.zeros((size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimming(img):\n",
    "    mask = img < 255\n",
    "    x = np.where(np.sum(mask, axis=0) > 1)[0]\n",
    "    y = np.where(np.sum(mask, axis=1) > 1)[0]\n",
    "    \n",
    "    x_min, x_max = x[0], x[-1]\n",
    "    y_min, y_max = y[0], y[-1]\n",
    "    return x_min, x_max, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_bytes_decode_img():\n",
    "    with BytesIO() as output:\n",
    "        func()\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_graph = cv2.imdecode(img_array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_player(img, mappable, img_diff):\n",
    "    height, width = img.shape[:2]\n",
    "    jet_color = np.array(cm.jet(img_diff/250)[:3])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    with BytesIO() as output:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"shogi_player\")\n",
    "        plt.imshow(cv2.rectangle(img, (0, 0), (width-1, height-1), jet_color * 255, int(min(height, width)/10)))\n",
    "        plt.colorbar(mappable, fraction=0.026, pad=0.04)\n",
    "        plt.savefig(output, format=\"PNG\")\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_player = cv2.imdecode(img_array, 3)\n",
    "    \n",
    "    return img_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_img(x_smooth, y_smooth, t):\n",
    "    with BytesIO() as output:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.plot(x_smooth/10, y_smooth, label=\"difference images\")\n",
    "        plt.hlines(17, 0, scores_df.shape[0]//10, label=\"threshold\")\n",
    "        plt.plot(t, y_smooth[t], \"ro\", markersize=4)        \n",
    "        plt.title(\"difference from original & predict images\")\n",
    "        plt.xlabel(\"Time(s)\")\n",
    "        plt.ylabel(\"Abnormaly Level(smoothing)\")\n",
    "        plt.legend()\n",
    "        plt.savefig(output, format=\"PNG\")\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_graph = cv2.imdecode(img_array, 3)\n",
    "    \n",
    "    return img_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compare_pose(img_orig, img_pred):\n",
    "    with BytesIO() as output:\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(5, 3))\n",
    "        for i, (img, title) in enumerate(zip([img_orig, img_pred], [\"original\", \"prediction\"])):  \n",
    "            ax[i].imshow(img, cmap=\"gray\")\n",
    "            ax[i].set_title(title)\n",
    "        plt.savefig(output, format=\"PNG\")\n",
    "        plt.close()\n",
    "        image_string = output.getvalue()\n",
    "    img_array = np.fromstring(image_string, np.uint8)\n",
    "    img_pose = cv2.imdecode(img_array, 3)\n",
    "    \n",
    "    return img_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画に対してリアルタイムに評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cap):\n",
    "    pose_thresh = 5 # openposeによるスケルトン抽出時の信頼度スコアのしきい値\n",
    "    sampling_frame_rate = 6\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))//sampling_frame_rate\n",
    "\n",
    "    # ヒートマップ用初期パラメータ\n",
    "    norm = Normalize(vmin=0, vmax=250)\n",
    "    cmap = cm.get_cmap(\"jet\")\n",
    "    mappable = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    mappable._A = []\n",
    "    \n",
    "    # 転送用設定\n",
    "#     s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#     s.bind((\"10.0.2.159\", 9898))\n",
    "#     print(\"Waiting...\")\n",
    "#     s.listen(1)\n",
    "#     soc_cli, addr = s.accept()\n",
    "#     print(\"Connected {}\".format(addr))\n",
    "    \n",
    "#     soc_sev = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#     soc_sev.connect((\"10.0.2.163\", 8989))\n",
    "#     print(\"Connected\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\")\n",
    "    video = cv2.VideoWriter(\"../abnormal_pose_detection_result.mp4\", fourcc, 20.0, (720, 480))\n",
    "    \n",
    "    # openposeモデルの読み込み\n",
    "    pose_detector = PoseDetector(\"posenet\", \"../utils/DeNAPose/models/coco_posenet.npz\", device=0)\n",
    "    \n",
    "    # アーキテクチャの読み込み\n",
    "    with open(\"../output/conv_vae_model_20190107_074233.json\") as f:\n",
    "        json_string = f.readline()\n",
    "\n",
    "    vae = model_from_json(json_string)\n",
    "    vae.load_weights(\"../output/conv_vae_best_param_20190107_074233.hdf5\")\n",
    "    \n",
    "    #while cap.isOpened():\n",
    "    for i in tqdm(range(total)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, sampling_frame_rate*i)\n",
    "        # 動画用に書き出すデータを1枚のcanvasにまとめる\n",
    "        canvas = np.ones((480, 720, 3), np.uint8) * 255\n",
    "        h_canvas, w_canvas = canvas.shape[:2]\n",
    "        \n",
    "        frame = cap.read()[1]\n",
    "        try:\n",
    "            img_orig, img_skelton = extract_skelton(frame, False, pose_detector, pose_thresh)\n",
    "        except:\n",
    "            continue\n",
    "        del frame\n",
    "        img_pose = preprocess(cv2.cvtColor(img_skelton, cv2.COLOR_RGB2GRAY)) / 255\n",
    "        del img_skelton\n",
    "        \n",
    "        h, w = img_pose.shape\n",
    "        img_pose = img_pose.reshape(1, h, w, 1)\n",
    "        img_pred = vae.predict(img_pose)[0].reshape(32,32)\n",
    "        img_pose = img_pose[0].reshape(32,32)\n",
    "        \n",
    "        img_diff = np.sum(np.abs(img_pose - img_pred))\n",
    "        \n",
    "        # openpose結果の出力\n",
    "        #img_heat = cv2.cvtColor(make_player(img_orig, mappable, img_diff), cv2.COLOR_BGR2RGB)\n",
    "        img_heat = make_player(img_orig, mappable, img_diff)\n",
    "        del img_orig\n",
    "        x, w, y, h = trimming(img_heat)\n",
    "        img_heat = img_heat[y-16:h+16, x-16:w+16]\n",
    "        h_orig, w_orig = img_heat.shape[:2]\n",
    "\n",
    "        canvas[0:h_orig, 0:w_orig] = img_heat\n",
    "        \n",
    "        # 対局中の局面評価を出力\n",
    "#         img_graph = make_graph_img(,i)\n",
    "\n",
    "#         h_result = h_canvas - img_heat.shape[0]\n",
    "#         h_graph, w_graph = img_graph.shape[:2]\n",
    "#         rate = round(h_result / h_graph, 2)\n",
    "#         img_graph = cv2.resize(img_graph.copy(), (int(w_graph*rate), h_result))\n",
    "#         h_graph, w_graph = img_graph.shape[:2]\n",
    "\n",
    "#         canvas[h_canvas - h_graph:h_canvas, 0:w_graph] = img_graph\n",
    "#         del img_graph\n",
    "        \n",
    "        # 入力データと予測データの比較画像を出力\n",
    "        img_compare = make_compare_pose(img_pose, img_pred)\n",
    "        del img_pose, img_pred\n",
    "\n",
    "        x, w, y, h = trimming(cv2.cvtColor(img_compare, cv2.COLOR_BGR2GRAY))\n",
    "        img_compare = img_compare[y-16:h+16, x-16:w+16]\n",
    "        h_comp, w_comp = img_compare.shape[:2]\n",
    "\n",
    "        #canvas[h_canvas - h_graph - 3:h_canvas, w_graph:w_graph + img_compare.shape[1]] = img_compare\n",
    "        canvas[h_canvas - h_comp:h_canvas, 0:w_comp] = img_compare\n",
    "        \n",
    "        video.write(canvas)\n",
    "        # サーバーへ転送\n",
    "        #canvas_string = canvas.tostring()\n",
    "        #soc_cli.send(canvas_string)\n",
    "        del canvas\n",
    "        #del canvas_string\n",
    "        del img_heat\n",
    "        del img_compare\n",
    "        \n",
    "        #soc_sev.recv(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c4e91edf4f47ab962c87c2125372c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=542), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/Desktop/Research/utils/DeNAPose/pose_detector.py:147: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  paf_in_edge = np.hstack([paf[0][np.hsplit(integ_points, 2)], paf[1][np.hsplit(integ_points, 2)]])\n",
      "/home/nvidia/.pyenv/versions/3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n",
      "/home/nvidia/.pyenv/versions/3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py:10: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"../data/final1_labeling.mp4\")\n",
    "main(cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
